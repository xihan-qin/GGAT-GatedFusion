{"cells":[{"cell_type":"markdown","metadata":{"id":"0quiQZnHBMnE"},"source":["## Mount Drive"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1668,"status":"ok","timestamp":1768760156564,"user":{"displayName":"ccqstd","userId":"13350588400665061144"},"user_tz":300},"id":"2MytS4OrlCCg","outputId":"cd90d996-c5ff-46e1-c5cf-6692fb362555"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# mount drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)\n","os.chdir('/content/drive/My Drive/Colab_Notebooks/github/GGAT-GatedFusion')"]},{"cell_type":"markdown","metadata":{"id":"K9guWYYibgve"},"source":["# For Torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4654,"status":"ok","timestamp":1768759575655,"user":{"displayName":"ccqstd","userId":"13350588400665061144"},"user_tz":300},"id":"WsrosFnHdafg","outputId":"e08a907e-4919-4990-abee-adefbf32dcc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.9.0+cu126\n"]}],"source":["import torch\n","print(torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8883,"status":"ok","timestamp":1768759584539,"user":{"displayName":"ccqstd","userId":"13350588400665061144"},"user_tz":300},"id":"iAJxQhkoEZ92","outputId":"8476b17f-5ccd-432e-fbd5-b7a517119933"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp312-cp312-linux_x86_64.whl (10.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp312-cp312-linux_x86_64.whl (5.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt26cu124-cp312-cp312-linux_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-geometric\n","  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.3.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2026.1.4)\n","Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n","Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\n","Successfully installed torch-cluster-1.6.3+pt26cu124 torch-geometric-2.7.0 torch-scatter-2.1.2+pt26cu124 torch-sparse-0.6.18+pt26cu124 torch-spline-conv-1.2.2+pt26cu124\n"]}],"source":["!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1768759588003,"user":{"displayName":"ccqstd","userId":"13350588400665061144"},"user_tz":300},"id":"-xS6LxSn2rSy","outputId":"5d3595a7-3134-4839-825e-f92625dd6f7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"]}],"source":["# Enable dynamic memory allocation in PyTorch to help with fragmentation\n","# Before importing torch\n","%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCpMHih_ozSY"},"outputs":[],"source":["os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"      # This will force CUDA to throw the error at the exact line it happens, not later at torch.tensor(...)."]},{"cell_type":"markdown","metadata":{"id":"YdFl3x2SBBFf"},"source":["# Package Section"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13209,"status":"ok","timestamp":1768759699975,"user":{"displayName":"ccqstd","userId":"13350588400665061144"},"user_tz":300},"id":"91DyaHYtki0w","outputId":"3ea1d6bc-8d37-4b1e-b57e-a836a27d4b53"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_scatter/_version_cuda.so\n","  import torch_geometric.typing\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_cluster/_version_cuda.so\n","  import torch_geometric.typing\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_spline_conv/_version_cuda.so\n","  import torch_geometric.typing\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_sparse/_version_cuda.so\n","  import torch_geometric.typing\n"]}],"source":["import time\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from sklearn.metrics import roc_auc_score\n","from torch_geometric.nn import GATConv\n","from torch_geometric.data import Data\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","import time\n","import sys\n","import scipy.sparse as sp\n","from torch_geometric.utils import from_scipy_sparse_matrix\n","import pickle\n","import networkx as nx\n","from torch_geometric.utils import from_networkx"]},{"cell_type":"markdown","metadata":{"id":"osJVG4CTbZgK"},"source":["# Functions"]},{"cell_type":"markdown","metadata":{"id":"Hx-j1vrLbuk8"},"source":["## Process data files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eUyoBs6CdQ92"},"outputs":[],"source":["def get_gene_idx_dict_from_file(node_file_name):\n","    f = open(node_file_name, \"r\")\n","    gene_idx_dict = {}\n","    idx = 0\n","    for line in f:\n","        node = line.strip()\n","        gene_idx_dict[node] = idx\n","        idx += 1\n","    f.close()\n","    return gene_idx_dict\n","\n","def get_disease_sets(file_path):\n","    dis_pairs = []   #[(disA, disB), ...]\n","    labels = []      # [label, ...]\n","    disease_genes_dict = {}     #{disease: [gene_1, gene_2, ...]}\n","\n","    f = open(file_path, \"r\")\n","    head = True\n","    for line in f:\n","        if head:\n","            head = False\n","            continue\n","\n","        row = line.strip().split(\"\\t\")\n","        dis_pair, disease_a_genes, disease_b_genes, all_genes, rr = row\n","\n","        disease_a, disease_b = dis_pair.split(\"&\")\n","\n","        dis_pairs.append((disease_a, disease_b))\n","        labels.append(int(rr))\n","\n","        disease_genes_dict[disease_a] = [int(gene) for gene in disease_a_genes.split(\",\")]\n","        disease_genes_dict[disease_b] = [int(gene) for gene in disease_b_genes.split(\",\")]\n","\n","\n","    f.close()\n","\n","    return dis_pairs, labels, disease_genes_dict\n","\n","def get_disease_pair_rr_list(dis_pairs, labels, disease_genes_dict, node_idx_dict):\n","    disease_pair_rr = []\n","    for idx in range(len(dis_pairs)):\n","        disease_a, disease_b = dis_pairs[idx]\n","        gene_list = disease_genes_dict[disease_a] + disease_genes_dict[disease_b]\n","        gene_list = [node_idx_dict[str(node)] for node in gene_list if str(node) in node_idx_dict]\n","        RR = labels[idx]\n","        disease_pair_rr.append([gene_list, RR])\n","    return disease_pair_rr\n","\n","def get_graph_from_file_and_map_ids(network_file, node_dict, **kwargs):\n","    \"\"\"\n","        generate a graph based on the input file\n","        The input file is provided by Joerg Menche et al. in their paper's supplementary\n","        Thus modify their function to parse the file and get the graph\n","        The function returns:\n","        G: the graph with self loop removed\n","    \"\"\"\n","\n","    defaultKwargs = {'self_link': True}\n","    kwargs = { **defaultKwargs, **kwargs}\n","\n","    G = nx.Graph()\n","    network_file = open(network_file,'r')\n","    for line in network_file:\n","        # lines starting with '#' will be ignored\n","        if line[0]=='#':\n","            continue\n","        line_data   = line.strip().split('\\t')\n","        gene1 = line_data[0]\n","        gene2 = line_data[1]\n","\n","        G.add_edge(node_dict[gene1],node_dict[gene2])\n","\n","    # remove self links\n","    if not kwargs['self_link']:\n","        remove_self_links(G)\n","    return G\n","#------------------------------------------------------------------------------#\n","def remove_self_links(G):\n","    sl = nx.selfloop_edges(G)\n","    G.remove_edges_from(sl)\n","\n","#------------------------------------------------------------------------------#\n","def file_to_matrix(file):\n","    matrix = np.loadtxt(file, delimiter='\\t')\n","    return matrix"]},{"cell_type":"markdown","metadata":{"id":"1tiKd3d0KhNA"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LP7aD2ul70fi"},"outputs":[],"source":["# GGAT + GRU for Disease Pair Prediction (RR labels) with Attention Pooling + Validation (yes/no)\n","\"\"\"\n","  Node Features + Graph → GGATGRU → Node Embeddings\n","                                  ↓\n","                            AttentionPooling\n","                                  ↓\n","                       Fully-Connected Predictor → Binary label\n","\"\"\"\n","\n","\n","# GRU\n","class GRUSubLayer(nn.Module):\n","    \"\"\"\n","      define layer GRUSub:\n","        h_dim: dimensions of the hidden state vector h\n","        h_in_dim: dimensions of the input vector h_in\n","    \"\"\"\n","    def __init__(self, h_dim, h_in_dim):\n","        super().__init__() # Initializes the parent class nn.Module, allows to use .to(), .eval(), etc. those build-in functions\n","        self.reset_gate = nn.Sequential(nn.Linear(h_dim + h_in_dim, h_in_dim), nn.Sigmoid())\n","        self.update_gate = nn.Sequential(nn.Linear(h_dim + h_in_dim, h_in_dim), nn.Sigmoid())\n","        self.transform = nn.Sequential(nn.Linear(h_dim + h_in_dim, h_in_dim), nn.Tanh())  # GRU uses Tanh\n","\n","    def forward(self, h, h_in):\n","        # Concatenates h and h_in then used as input for the gates\n","        # dim = 0, concat batch dim; dim = 1, concat features\n","        a = torch.cat((h, h_in), dim=1)\n","        r = self.reset_gate(a) # apply reset gate to attentions a\n","        z = self.update_gate(a) # apply update gate to attentions a\n","        joined = torch.cat((h, r * h_in), dim=1) # Element-wise multiplication to get the weighted h_in and concat with h\n","        h_hat = self.transform(joined)  # apply Tanh\n","        return (1 - z) * h_in + z * h_hat # GRU\n","\n","# GGAT + GRU\n","class GGATGRU(nn.Module):\n","    \"\"\"\n","      defube module GGATGRU:\n","        input -> input_droupout -> GATConv1 (with gat_dropout) -> ELU activation -> out1\n","              |                                             |\n","              --------------------------------------------------> GRU1 -> x1\n","\n","          x1  -> input_droupout -> GATConv2 (with gat_dropout) -> ELU activation -> Linear(dimTrans) -> out2\n","              |                                                                 |\n","              ----------------------------------------------------------------------> GRU2 -> x2\n","\n","          x2  -> input_droupout -> GATConv3 (with gat_dropout) -> ELU activation -> Linear(dimTrans) -> out3\n","              |                                                                 |\n","              ----------------------------------------------------------------------> GRU3 -> x3\n","      features:\n","        input_dropout:\n","          For rich input features to prevents the model from overly relying on specific input feature dimensions and help generalize better.\n","          Set to 0 when the input is already highly sparse, doing so may discard too much information by ignoring critical ones for the task\n","        heads: higher is better to learn diverse attention but higher memory.\n","        gat_dropout: Encourages to consider a wider variety of neighbors rather than fixating on a few strong edges. Used when the task is binary classification and sensitive to overconfidence.\n","    \"\"\"\n","    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, input_dropout = 0.2, gat_dropout=0.4):\n","        super().__init__()\n","        self.heads1 = heads                          # original\n","        self.heads2 = round(heads / 2)              # reduced\n","        self.input_dropout = input_dropout\n","        self.gat_dropout = gat_dropout\n","        self.heads = heads\n","        self.hidden_channels = hidden_channels\n","        self.in_channels = in_channels\n","\n","        self.transform = nn.Linear(in_channels, hidden_channels * heads) if in_channels != hidden_channels * heads else nn.Identity()\n","\n","        self.gat1 = GATConv(hidden_channels * self.heads1, hidden_channels, heads=self.heads1, dropout=gat_dropout)       # GAT1 input dim: hidden_channels * heads1, output dim: hidden_channels * heads1\n","        self.gat2 = GATConv(hidden_channels * self.heads1, hidden_channels, heads=self.heads2, dropout=gat_dropout)       # GAT2 input dim: hidden_channels * heads2, output dim: hidden_channels * heads2\n","        self.gat3 = GATConv(hidden_channels * self.heads2, out_channels, heads=1, dropout=gat_dropout)\n","\n","        # GRU input/output alignment\n","        self.gru1 = GRUSubLayer(h_dim=hidden_channels * self.heads1, h_in_dim=hidden_channels * self.heads1)\n","        # change dimensions: [N, hidden_channels * heads] -> [N, hidden_channels]\n","        self.pre_gat2 = nn.Linear(hidden_channels * self.heads1, hidden_channels * self.heads2)\n","        self.gru2 = GRUSubLayer(h_dim=hidden_channels * self.heads2, h_in_dim=hidden_channels * self.heads2)\n","\n","        # self.trans3 = nn.Linear(hidden_channels, hidden_channels)\n","        self.pre_gat3 = nn.Linear(hidden_channels * self.heads2, out_channels)\n","        self.gru3 = GRUSubLayer(h_dim=out_channels, h_in_dim=out_channels)\n","\n","    def forward(self, x, edge_index):\n","        x = self.transform(x)\n","\n","        h1 = x\n","        x = F.dropout(x, p=self.input_dropout, training=self.training)\n","        x = F.elu(self.gat1(x, edge_index))\n","        x = self.gru1(x, h1)  # after gru1: [N, hidden_channels * heads]\n","\n","        h2 = x # h2: [N, hidden_channels * heads]\n","        x = F.dropout(x, p=self.input_dropout, training=self.training)\n","        x = F.elu(self.gat2(x, edge_index)) # now, x after gat2: [N, hidden_channels]\n","        x = self.gru2(x, self.pre_gat2(h2)) # need to transfer h2's dimentions to the same as x\n","\n","        # h3 = self.trans3(x)\n","        # x = self.pre_gat3(x)\n","        h3 = x\n","        x = F.dropout(x, p=self.input_dropout, training=self.training)\n","        x = self.gat3(x, edge_index)\n","        x = self.gru3(x, self.pre_gat3(h3))\n","        return x\n","\n","# RELUGate\n","class RELUSubLayer(nn.Module):\n","    \"\"\"\n","      define layer GRUSub:\n","        h_dim: dimensions of the hidden state vector h\n","        h_in_dim: dimensions of the input vector h_in\n","    \"\"\"\n","    def __init__(self, h_dim, h_in_dim):\n","        super().__init__() # Initializes the parent class nn.Module, allows to use .to(), .eval(), etc. those build-in functions\n","        self.reset_gate = nn.Sequential(nn.Linear(h_dim + h_in_dim, h_in_dim), nn.Sigmoid())\n","        self.update_gate = nn.Sequential(nn.Linear(h_dim + h_in_dim, h_in_dim), nn.Sigmoid())\n","        self.transform = nn.Sequential(nn.Linear(h_dim + h_in_dim, h_in_dim), nn.ReLU())  # GRU uses Tanh, here use RELU\n","\n","    def forward(self, h, h_in):\n","        # Concatenates h and h_in then used as input for the gates\n","        # dim = 0, concat batch dim; dim = 1, concat features\n","        a = torch.cat((h, h_in), dim=1)\n","        r = self.reset_gate(a) # apply reset gate to attentions a\n","        z = self.update_gate(a) # apply update gate to attentions a\n","        joined = torch.cat((h, r * h_in), dim=1) # Element-wise multiplication to get the weighted h_in and concat with h\n","        h_hat = self.transform(joined)  # apply Tanh\n","        return (1 - z) * h_in + z * h_hat # GRU\n","\n","# GGAT + RELUGate\n","class GGATRELU(nn.Module):\n","    \"\"\"\n","      defube module GGATGRU:\n","        input -> input_droupout -> GATConv1 (with gat_dropout) -> ELU activation -> out1\n","              |                                             |\n","              --------------------------------------------------> GRU1 -> x1\n","\n","          x1  -> input_droupout -> GATConv2 (with gat_dropout) -> ELU activation -> Linear(dimTrans) -> out2\n","              |                                                                 |\n","              ----------------------------------------------------------------------> GRU2 -> x2\n","\n","          x2  -> input_droupout -> GATConv3 (with gat_dropout) -> ELU activation -> Linear(dimTrans) -> out3\n","              |                                                                 |\n","              ----------------------------------------------------------------------> GRU3 -> x3\n","      features:\n","        input_dropout:\n","          For rich input features to prevents the model from overly relying on specific input feature dimensions and help generalize better.\n","          Set to 0 when the input is already highly sparse, doing so may discard too much information by ignoring critical ones for the task\n","        heads: higher is better to learn diverse attention but higher memory.\n","        gat_dropout: Encourages to consider a wider variety of neighbors rather than fixating on a few strong edges. Used when the task is binary classification and sensitive to overconfidence.\n","    \"\"\"\n","    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, input_dropout = 0.2, gat_dropout=0.4):\n","        super().__init__()\n","        self.input_dropout = input_dropout\n","        self.gat_dropout = gat_dropout\n","        self.heads = heads\n","        self.hidden_channels = hidden_channels\n","        self.in_channels = in_channels\n","\n","        self.transform = nn.Linear(in_channels, hidden_channels * heads) if in_channels != hidden_channels * heads else nn.Identity()\n","\n","        self.gat1 = GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=gat_dropout)   # GAT output dim: hidden_channels * heads\n","        self.gat2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=gat_dropout)\n","        self.gat3 = GATConv(hidden_channels, out_channels, heads=1, dropout=gat_dropout)\n","\n","\n","        self.relu1 = RELUSubLayer(h_dim=hidden_channels * heads, h_in_dim=hidden_channels * heads)\n","        # change dimensions: [N, hidden_channels * heads] -> [N, hidden_channels]\n","        self.pre_gat2 = nn.Linear(hidden_channels * heads, hidden_channels)\n","        self.relu2 = RELUSubLayer(h_dim=hidden_channels, h_in_dim=hidden_channels)\n","\n","        self.trans3 = nn.Linear(hidden_channels, hidden_channels)\n","        self.pre_gat3 = nn.Linear(hidden_channels, hidden_channels)\n","        self.relu3 = RELUSubLayer(h_dim=out_channels, h_in_dim=hidden_channels)\n","\n","    def forward(self, x, edge_index):\n","        x = self.transform(x)\n","\n","        h1 = x\n","        x = F.dropout(x, p=self.input_dropout, training=self.training)\n","        x = F.elu(self.gat1(x, edge_index))\n","        x = self.relu1(x, h1)  # after gru1: [N, hidden_channels * heads]\n","\n","        h2 = x # h2: [N, hidden_channels * heads]\n","        x = F.dropout(x, p=self.input_dropout, training=self.training)\n","        x = F.elu(self.gat2(x, edge_index)) # now, x after gat2: [N, hidden_channels]\n","        x = self.relu2(x, self.pre_gat2(h2)) # need to transfer h2's dimentions to the same as x\n","\n","        h3 = self.trans3(x)\n","        x = self.pre_gat3(x)\n","        x = F.dropout(x, p=self.input_dropout, training=self.training)\n","        x = self.gat3(x, edge_index)\n","        x = self.relu3(x, h3)\n","        return x\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"L2W_ZHQOKZY2","executionInfo":{"status":"ok","timestamp":1768760345095,"user_tz":300,"elapsed":40,"user":{"displayName":"ccqstd","userId":"13350588400665061144"}}},"outputs":[],"source":["class AttentionPooling(nn.Module):\n","    \"\"\"\n","      define AttentionPooling:\n","        att_mlp: A 2-layer MLP (Multi-Layer Perceptron) that computes a scalar attention score for each node (which nodes are important overall)\n","        softmax: Normalizes scores into probabilities\n","\n","    \"\"\"\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.att_mlp = nn.Sequential(\n","            nn.Linear(input_dim, input_dim*2),  # expands the representation to a higher-dimensional attention space, allowing the MLP to learn richer interactions between features.\n","            nn.Tanh(),\n","            nn.Linear(input_dim*2, 1)\n","        )\n","\n","    def forward(self, node_embs):\n","        attn_weights = self.att_mlp(node_embs)\n","        attn_weights = torch.softmax(attn_weights, dim=0) # Ensures attention scores are positive and sum to 1.\n","        pooled = (attn_weights * node_embs).sum(dim=0)\n","        # Output is a single vector: the weighted average of all nodes based on learned attention.\n","        return pooled # shape: [num_nodes, 1] → score for each node.\n","\n","def build_rr_predictor(input_dim, hidden_dim=16, output_dim=1):\n","    \"\"\"\n","      No custom logic, no need to define init and forward.\n","      just use nn.Sequential to stack layers.\n","    \"\"\"\n","    return nn.Sequential(\n","        nn.ReLU(),\n","        nn.Linear(input_dim, hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(hidden_dim, output_dim)\n","    )"]},{"cell_type":"markdown","metadata":{"id":"UGyDfTsjLl1O"},"source":["## Helper functions for Training and Testing"]},{"cell_type":"markdown","metadata":{"id":"xYjGMZrwf89V"},"source":["### Adapted from the GGAT-cancer to follow a similar loading and normalization procedure\n","\n","link: https://github.com/lhanlhanlhan/ggat\n","\n","Note that GGAT-cancer is designed for **node classification**, whereas our task\n","is **disease pair prediction**. Moreover, the GGAT-cancer framework is\n","not scalable to datasets of the size used in our study. As a result, we do not\n","adapt our model to a node classification setting nor perform direct comparisons\n","with GGAT-cancer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Ls6BZwBs8oV"},"outputs":[],"source":["def normalize_features(mx):\n","    \"\"\"\n","      Row-normalize sparse matrix\n","    \"\"\"\n","    rowsum = np.array(mx.sum(1)) # sum of each row\n","\n","    # for rowsum = 0, replace with epsilon to avoid division by zero\n","    epsilon = 1e-10\n","    rowsum_safe = np.where(rowsum == 0, epsilon, rowsum)\n","\n","    r_inv = np.power(rowsum_safe, -1).flatten() # 1 / rowsum, then flattern to 1d array\n","    r_inv[np.isinf(r_inv)] = 0.   # in case there is still infinities due to division , set to 0.\n","\n","    r_mat_inv = sp.diags(r_inv)   # diagonal matrix with 1/rowsum\n","    mx = r_mat_inv.dot(mx)        # multiplicates the matrix itself, so that the matrix is scaled by 1/rowsum\n","    return mx\n","\n","def load_data(path=\"./data/dis/\", dataset=\"dis\"):\n","    \"\"\"\n","      [modified for our data] from GGAT cancer to make sure fair comparison\n","    \"\"\"\n","    print('Loading {} dataset...'.format(dataset))\n","\n","    # [node id, embedding, label] ... for dis set, there is no label, all are place holder -1\n","    idx_features_labels = np.genfromtxt(f\"{path}{dataset}.content\", dtype=np.dtype(str))\n","    # extract features: from columns idx 1 to -1, use sparse CSR matrix\n","    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n","\n","    # build graph\n","    idx = np.array(idx_features_labels[:, 0], dtype=np.int32) # node ids, cloumn idx 0\n","    idx_map = {j: i for i, j in enumerate(idx)}   # {node id: in file row id}\n","\n","    # edges from file: [[nodei,nodej],...]\n","    edges_unordered = np.genfromtxt(f\"{path}{dataset}.cites\", dtype=np.int32)\n","    # falttern the list, then map the node ids with in file row ids, then reshape back to [[rowi,rowj],...]\n","    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n","\n","    # coo_matrix((list-of-data, (row_indices, col_indices)), shape=(N, N))\n","    # adj is a sparse matrix with [i,j] = 1\n","    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n","                        shape=(features.shape[0], features.shape[0]), dtype=np.float32)\n","    # build symmetric adjacency matrix, adds [j,i] = 1\n","    # (adj.T > adj) marks the edges that reverse edges exist but forward ones don’t (if has weights, then check larger weights and mark True)\n","    # adj.T.multiply(adj.T > adj) does element-wise multiplication to add those edges.\n","    # keep \"- adj.multiply(adj.T > adj)\" for future use when edge has different weights, so for each edge-pair (i, j) and (j, i), keep the higher weight\n","    # this will not affect unweighted edges, adj.multiply(adj.T > adj) will get all 0s.\n","    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n","\n","    features = normalize_features(features)\n","    # converts the sparse matrix into a dense NumPy matrix\n","    # then np.matrix into a standard np.ndarray, finally convert to torch tensors with float32\n","    features = torch.FloatTensor(np.array(features.todense()))\n","\n","    # from_scipy_sparse_matrix => PyTorch Geometric (PyG) format: edge_index, edge_weight\n","    # edge_index: tensor of shape [2, num_edges], represents the edge list in COO format.\n","    # edge_weight: tensor of the weights on the edges, not used for unweighted graph\n","    edge_index, _ = from_scipy_sparse_matrix(adj)\n","    return features, edge_index\n"]},{"cell_type":"markdown","metadata":{"id":"yGEGPSMc6-VH"},"source":["## Mine"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":160,"status":"ok","timestamp":1768760310332,"user":{"displayName":"ccqstd","userId":"13350588400665061144"},"user_tz":300},"id":"vtbwzDZEKlwj"},"outputs":[],"source":["def log_print(message, log_file):\n","    \"\"\"\n","      used to print the training log and write into files\n","    \"\"\"\n","    print(message)\n","    log_file.write(message + \"\\n\")\n","    log_file.flush()  # flush the buffer, writes to the disk immediately.\n","\n","\n","def prepare_data(path, dataset_name):\n","    features, edge_index = load_data(path, dataset_name)\n","    x = features.clone().detach().float()\n","    edge_index = edge_index.clone().detach().long()\n","    return Data(x=x, edge_index=edge_index), x\n","\n","def prepare_models_and_data(args):\n","    \"\"\"\n","      define models and prep data based on the args\n","      1. label: label_model, label_data\n","      2. n2v: n2v_model, n2v_data\n","    \"\"\"\n","    # init the objs, they are needed for defined functions evenif the current mode doesn't need it, still need to set to None.\n","    # None is immutable in Python, this does not cause problems when later assign new values to any of them individually, they will point to others instead of None\n","    n2v_data = label_data = n2v_model = label_model = None\n","\n","    if args.model_type == \"n2v\":\n","        n2v_data, n2v_x = prepare_data('./data/dis/', 'dis')\n","        n2v_model = build_ggat(n2v_x.shape[1], args)\n","\n","    if args.model_type == \"label\":\n","        label_data, label_x = prepare_data('./data/dis/', 'label2vec')  # 'dishot', 'label2vec', 'label2vec_autoencoder'\n","        label_model = build_ggat(label_x.shape[1], args)\n","\n","    return label_data, label_model, n2v_data, n2v_model\n","\n","def build_ggat(input_dim, args):\n","  if args.model == \"GGATGRU\":\n","    return build_ggatgru(input_dim, args)\n","\n","def build_ggatgru(input_dim, args):\n","    \"\"\"\n","      used to build 2 models: label_model and n2v_model\n","    \"\"\"\n","    return GGATGRU(\n","        in_channels=input_dim,\n","        hidden_channels=args.hidden,\n","        out_channels=args.inter_dim,\n","        heads=args.nb_heads,\n","        input_dropout=args.input_dropout,\n","        gat_dropout=args.gat_dropout\n","    )\n","\n","def to_device(*objs, device):\n","    \"\"\"\n","      send the obj to the given device\n","    \"\"\"\n","    return [t.to(device) if t is not None else t for t in objs]\n","\n","def get_model_params(args, label_model=None, n2v_model=None,\n","                     attention_pooler=None, rr_predictor=None):\n","    \"\"\"\n","      add model parameters for optimizer based on the model option\n","    \"\"\"\n","    model_params = []\n","\n","    if args.model_type == \"label\":\n","        model_params += list(label_model.parameters())\n","    if args.model_type == \"n2v\":\n","        model_params += list(n2v_model.parameters())\n","\n","    # Always include attention pooler and predictor\n","    model_params += list(attention_pooler.parameters())\n","    model_params += list(rr_predictor.parameters())\n","    return model_params\n","\n","def split_train_val(gene_lists, labels, use_valid=True, test_size=0.1, seed=42, log_fn=print):\n","    \"\"\"\n","      If choose to include the validation set:\n","      stratified train/val split, default split: 0.9 train, 0.1 valid, seed: 42\n","      If not:\n","      returns full data as train_set and empty val_set.\n","\n","      Parameters:\n","          gene_lists: List of gene sets\n","          use_valid: validation set or not\n","          test_size: size of validation set\n","          seed: Random seed\n","          log_fn: defined logging function, print log and write to file\n","\n","      Returns:\n","          train_set (tuples): (gene_list, label)\n","          val_set (tuples): (gene_list, label)\n","    \"\"\"\n","    if use_valid:\n","        train_gene, val_gene, train_label, val_label = train_test_split(\n","            gene_lists, labels, test_size=test_size, stratify=labels, random_state=seed\n","        )\n","        # zip genes and labels into paired lists for iteration.\n","        train_set = list(zip(train_gene, train_label))\n","        val_set = list(zip(val_gene, val_label))\n","        # check for balance: how many 0 and 1\n","        log_fn(f\"Train Label Distribution: {Counter(train_label)}\")\n","        log_fn(f\"Val Label Distribution: {Counter(val_label)}\")\n","    else:\n","        train_set = list(zip(gene_lists, labels))\n","        val_set = []\n","        log_fn(f\"Train Label Distribution: {Counter(labels)}\")\n","\n","    return train_set, val_set\n","\n","def set_model_mode(mode, args, label_model, n2v_model, rr_predictor, attention_pooler):\n","    \"\"\"\n","      helps to set all the models into the same mode: train or eval\n","      models include:\n","      label_model\n","      n2v_model\n","      rr_predictor\n","      attention_pooler\n","    \"\"\"\n","    if args.model_type == \"label\" and label_model:\n","        getattr(label_model, mode)()  # use () at the end to call the method, without () will just get the method not runing it.\n","    if args.model_type == \"n2v\" and n2v_model:\n","        getattr(n2v_model, mode)()\n","    getattr(rr_predictor, mode)()\n","    getattr(attention_pooler, mode)()\n","\n","def compute_node_embeddings(args, label_model, n2v_model, label_data, n2v_data):\n","    \"\"\"\n","      construct the embedding:\n","      1. label mode: only get the embedding output from label_model\n","      2. n2v mode: only get the embedding output from n2v_model\n","    \"\"\"\n","    if args.model_type == \"label\":\n","        return label_model(label_data.x, label_data.edge_index)\n","    elif args.model_type == \"n2v\":\n","        return n2v_model(n2v_data.x, n2v_data.edge_index)\n","    else:\n","        raise ValueError(f\"Unknown model_type: {args.model_type}\")\n","\n","def embed_disease_pairs(node_embeddings, pair_set, attention_pooler):\n","    \"\"\"\n","      each gene_list for each disease pair -> gene_embeddings -> attention score\n","      embs: list of attention scores for each disease pair\n","      labels: list of rr labels\n","    \"\"\"\n","    embs, labels = [], []\n","    for gene_list, rr_label in pair_set:\n","        try:\n","            gene_embs = node_embeddings[gene_list]\n","            embs.append(attention_pooler(gene_embs)) # get attention score\n","            labels.append(rr_label)\n","        except Exception as e:\n","            print(\"!\", e)\n","    # Stacks disease embeddings into a batch: [batch_size, hidden_dim]\n","    # Converts labels to a tensor and reshapes to [batch_size, 1]\n","    return torch.stack(embs), torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n","\n","def compute_metrics(logits, labels_tensor):\n","    \"\"\"\n","      Converts logits to probabilities and computes accuracy and AUC.\n","      Assumes binary classification (sigmoid + threshold 0.5).\n","    \"\"\"\n","    # Binary classification: outputs the probability of class 1.\n","    # Applies the sigmoid activation function to convert the raw logits into probabilities in the range (0, 1).\n","    # .view(-1): Flattens the tensor from [N,1] into a 1D array of shape [N], E.g., turns [[0.7], [0.3]] into [0.7, 0.3]\n","    # .cpu(): Moves the tensor from GPU to CPU memory. NumPy arrays can only live on the CPU.\n","    # .numpy(): converts the PyTorch tensor into a NumPy array\n","    probs = torch.sigmoid(logits).view(-1).cpu().numpy()\n","    labels_np = labels_tensor.view(-1).cpu().numpy()  # reshape the tensor from [N, 1] to [N] send to cpu and turn into numpy array\n","    # True for values > 0.5, and False otherwise, then transfer boolean type to fload type\n","    preds = (probs > 0.5).astype(np.float32)\n","    # val_probs, val_pred vs ground truth labels (val_labels_np)\n","    acc = (preds == labels_np).mean() # acc\n","    # if len(np.unique(val_labels_np)) == 2: checks if both classes (0 and 1) are present.\n","    # If only one class is present (e.g., all labels are 1s), AUC is undefined since AUC requires both positive and negative samples to compute the trade-off between TPR and FPR.\n","    # --> then sets AUC to NaN (Not a Number).\n","    auc = roc_auc_score(labels_np, probs) if len(np.unique(labels_np)) == 2 else float('nan') # AUC\n","    return acc, auc\n","\n","def evaluate_on_validation(args, label_model, n2v_model, rr_predictor, attention_pooler,\n","                           label_data, n2v_data, val_set, device):\n","    \"\"\"\n","      evaluation\n","    \"\"\"\n","    # set to validation mode so that:\n","    # 1. Dropout won't randomly zeroes out parts of the embeddings during validation (through nn.Dropout)\n","    # 2. BatchNorm doesn't update its running mean and variance (nn.BatchNorm) -- BatchNorm is not used in this script yet\n","    set_model_mode('eval', args, label_model, n2v_model, rr_predictor, attention_pooler)\n","    # disables gradient tracking globally inside the block\n","    # if not gradients will be computed but not used, this wastes memory and compute\n","    with torch.no_grad():\n","        node_embeddings = compute_node_embeddings(args, label_model, n2v_model, label_data, n2v_data)\n","\n","        val_embs, val_labels = embed_disease_pairs(node_embeddings, val_set, attention_pooler)\n","        val_embs, val_labels = to_device(val_embs, val_labels, device = device)\n","\n","        val_logits = rr_predictor(val_embs)\n","        val_acc, val_auc = compute_metrics(val_logits, val_labels)\n","        return val_acc, val_auc\n","\n","def save_best_states(args, rr_predictor, attention_pooler, label_model=None, n2v_model=None):\n","    \"\"\"\n","      construct the dictionary of best model state_dicts based on model_type.\n","    \"\"\"\n","    best_states = {\n","        'predictor_state_dict': rr_predictor.state_dict(),\n","        'pooler_state_dict': attention_pooler.state_dict()\n","    }\n","    if args.model_type == \"label\" and label_model:\n","        best_states['label_model_state_dict'] = label_model.state_dict()\n","    if args.model_type == \"n2v\" and n2v_model:\n","        best_states['n2v_model_state_dict'] = n2v_model.state_dict()\n","    return best_states"]},{"cell_type":"markdown","metadata":{"id":"imgM52ltduma"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"Mi1yAP4BLsO6"},"source":["## Set Parameters"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"YPZfEdvjdd_-","executionInfo":{"status":"ok","timestamp":1768760512210,"user_tz":300,"elapsed":3,"user":{"displayName":"ccqstd","userId":"13350588400665061144"}}},"outputs":[],"source":["dataset = 'RR1'\n","fold = 5\n","node_file_path = f'data/{dataset}/interactom_nodes.txt'   # stores the nodes for the largest connected component in human Interactome\n","edge_list_file_path = f'data/{dataset}/interactom_edges.txt' # stores the edges for the largest connected component in human Interactome\n","train_file_path = f'data/{dataset}/Fold{fold}/train_set.tsv'\n","test_file_path = f'data/{dataset}/Fold{fold}/test_set.tsv'"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"fFaSffHWLw-h","executionInfo":{"status":"ok","timestamp":1768760512264,"user_tz":300,"elapsed":17,"user":{"displayName":"ccqstd","userId":"13350588400665061144"}}},"outputs":[],"source":["class Args:\n","  model = \"GGATGRU\" # \"GGATGRU\"\n","  model_type = \"label\"  # options: \"label\", \"n2v\"\n","  no_cuda = False\n","  seed = 42\n","  epochs = 3000\n","  lr = 0.005\n","  weight_decay = 5e-4\n","  hidden = 8 # per_head\n","  nb_heads = 8\n","  inter_dim = 32  # added to control the dims between different layers\n","  input_dropout = 0 # 0.6 may be too high, tried 0.3 with 500, currently set to 0 to stop input_dropout\n","  gat_dropout = 0.4  # seperate gat with input\n","  dataset = \"dis\"\n","  use_valid = True\n","  earlystop = False\n","  early_stop_patience = 3000  # stop if no improvement after x epochs: 20, 100, 200, 500, currently no early stop\n","  min_delta = 1e-4  # Only count as improvement if gain is > 0.0001\n","\n","# 0. get all the args\n","args = Args()\n","\n","# training log file\n","log_file = open(f\"results/{dataset}/Fold{fold}/{dataset}_fold{fold}_training_log_{args.model}_{args.model_type}_{args.epochs}epochs_{args.inter_dim}inter.txt\", \"w\")\n","\n","# to save models\n","best_model_path = f'results/{dataset}/Fold{fold}/{dataset}_fold{fold}_best_model_{args.model}_{args.model_type}_{args.epochs}epochs_{args.inter_dim}inter.pt'\n","last_model_path = f'results/{dataset}/Fold{fold}/{dataset}_fold{fold}_last_model_{args.model}_{args.model_type}_{args.epochs}epochs_{args.inter_dim}inter.pt'\n","\n","# to save the predictions\n","best_model_output_file=f'results/{dataset}/Fold{fold}/{dataset}_fold{fold}_best_model_test_predictions_{args.model}_{args.model_type}_{args.epochs}epochs_{args.inter_dim}inter.tsv'\n","last_model_output_file=f'results/{dataset}/Fold{fold}/{dataset}_fold{fold}_last_model_test_predictions_{args.model}_{args.model_type}_{args.epochs}epochs_{args.inter_dim}inter.tsv'"]},{"cell_type":"markdown","metadata":{"id":"9JJzZQbgQLA5"},"source":["## Load Data Set"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"376f570MdHzw","executionInfo":{"status":"ok","timestamp":1768760513065,"user_tz":300,"elapsed":806,"user":{"displayName":"ccqstd","userId":"13350588400665061144"}}},"outputs":[],"source":["# 1. get graph original nodes\n","node_idx_dict = get_gene_idx_dict_from_file(node_file_path)\n","node_gene_dict = {v:k for k,v in node_idx_dict.items()}\n","# print(node_idx_dict)\n","\n","# 2. get selected disease pairs\n","# [(disA, disB), ...], [label, ...], {disease: [gene_1, gene_2, ...]}]\n","train_dis_pairs, train_labels, train_disease_genes_dict = get_disease_sets(train_file_path)\n","test_dis_pairs, test_labels, test_disease_genes_dict = get_disease_sets(test_file_path)\n","train_disease_pair_rr = get_disease_pair_rr_list(train_dis_pairs, train_labels, train_disease_genes_dict, node_idx_dict)\n","test_disease_pair_rr = get_disease_pair_rr_list(test_dis_pairs, test_labels, test_disease_genes_dict, node_idx_dict)\n","# print(train_disease_pair_rr[0])\n"]},{"cell_type":"markdown","metadata":{"id":"UPdinzPu0vGj"},"source":["## Prep Model and Data"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2536,"status":"ok","timestamp":1768760515611,"user":{"displayName":"ccqstd","userId":"13350588400665061144"},"user_tz":300},"id":"vMg05SjC0rec","outputId":"5dc5e7be-b305-48f8-e444-bced21d0c829"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading label2vec dataset...\n","Train Label Distribution: Counter({1: 5078, 0: 3624})\n","Val Label Distribution: Counter({1: 564, 0: 403})\n"]}],"source":["args.cuda = not args.no_cuda and torch.cuda.is_available() # Uses GPU if (CUDA is not explicitly disabled by the user) and (it is available)\n","device = 'cuda' if args.cuda else 'cpu' # Stores the current device, need it when constructing tensors or models on the same device.\n","\n","# fixed random seed, initialize the global RNG (random number generator) used by PyTorch, includes:\n","# 1. Weight initialization in layers like nn.Linear, GATConv, etc.\n","# 2. Random dropout masks (from nn.Dropout or F.dropout)\n","# 3. Tensor-level randomness (e.g., torch.rand, torch.randn)\n","# 4. Shuffling in PyTorch DataLoader if generator=torch.Generator().manual_seed(...) is set\n","torch.manual_seed(args.seed)\n","\n","# prep GGATGRU models and data based on the model choice\n","label_data, label_model, n2v_data, n2v_model = prepare_models_and_data(args)\n","\n","# MLP for rr label prediction\n","rr_predictor = build_rr_predictor(args.inter_dim)\n","\n","# to get attention score for each gene\n","attention_pooler = AttentionPooling(args.inter_dim)\n","\n","# loss function\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","# If use cuda: move the model and data to CUDA.\n","label_model, label_data, n2v_model, n2v_data, rr_predictor, attention_pooler = to_device(\n","    label_model, label_data, n2v_model, n2v_data, rr_predictor, attention_pooler, device = device)\n","\n","# Adam optimizer to update parameters during training: set learning rate, applies L2 regularization with 'weight_decay'\n","model_params = get_model_params(args, label_model, n2v_model,\n","                     attention_pooler, rr_predictor)\n","optimizer = optim.Adam(\n","    model_params,\n","    lr=args.lr,\n","    weight_decay=args.weight_decay\n",")\n","\n","# train_disease_pair_rr = [(gene_list, label), ...]\n","gene_lists = [x[0] for x in train_disease_pair_rr]\n","labels = [x[1] for x in train_disease_pair_rr]\n","\n","# stratified train-validation split\n","log_fn = lambda msg: log_print(msg, log_file)\n","\n","train_set, val_set = split_train_val(\n","    gene_lists, labels, use_valid=args.use_valid, log_fn=log_fn\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH1nLjxmrxzl"},"outputs":[],"source":["print(dataset)\n","print(fold)\n","print(args.model)\n","print(args.model_type)\n","print(args.epochs)"]},{"cell_type":"markdown","metadata":{"id":"hgKpuXlR0z2v"},"source":["## Train and Valid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04e3Q9eZ7OD4"},"outputs":[],"source":["best_val_auc = -1   # for choosing best model to save\n","patience_counter = 0   # for early stop\n","\n","\n","start_time = time.time()  # Start timing\n","\n","for epoch in range(args.epochs):\n","    # set the models to training mode\n","    set_model_mode('train', args, label_model, n2v_model, rr_predictor, attention_pooler)\n","\n","    # forward pass\n","    optimizer.zero_grad() # clears old gradients from previous steps.\n","    # get embeddings from label_model / n2v_model / fusion_model\n","    node_embeddings = compute_node_embeddings(args, label_model, n2v_model, label_data, n2v_data)\n","    # construct embeddings for disease pairs send to device\n","    train_embs, train_labels = embed_disease_pairs(node_embeddings, train_set, attention_pooler)\n","    train_embs, train_labels = to_device(train_embs, train_labels, device = device)\n","\n","    # print(train_embs.shape)\n","    logits = rr_predictor(train_embs) # feeds the disease embeddings into the MLP to predict a scalar logit\n","    loss = loss_fn(logits, train_labels) # binary cross-entropy loss\n","\n","    # backward pass\n","    # trace backward the gradient (how much the parameter affect the loss) = ∂loss/∂parameter(weight, biases)\n","    # use the chain rule for all used models -> stores in param.grad\n","    loss.backward()\n","    optimizer.step()  # updates parameters using those gradients\n","\n","    # evaluate on the training data for acc and auc\n","    with torch.no_grad():\n","        train_acc, train_auc = compute_metrics(logits, train_labels)\n","\n","    # :03d: formats the number to be 3 digits with leading zeros if needed.\n","    # :.4f: float to 4 decimal places\n","    log_msg = (f\"Epoch {epoch+1:03d} | Train Loss: {loss.item():.4f} | \"\n","            f\"Train Acc: {train_acc:.4f} | Train ROC AUC: {train_auc:.4f}\")\n","\n","    # valid if needed\n","    if args.use_valid:\n","        val_acc, val_auc = evaluate_on_validation(args, label_model, n2v_model,\n","            rr_predictor, attention_pooler, label_data, n2v_data, val_set, device)\n","        log_msg += f\" | Val Acc: {val_acc:.4f} | Val ROC AUC: {val_auc:.4f}\"\n","        score = val_auc\n","    else:\n","        score = train_auc\n","\n","    # print and save the log message\n","    log_print(log_msg, log_file)\n","\n","    # best model & early stop (if enabled)\n","    if score > best_val_auc + args.min_delta:\n","        best_val_auc = score\n","        patience_counter = 0\n","        best_states = save_best_states(args, rr_predictor, attention_pooler,\n","            label_model=label_model,  n2v_model=n2v_model)\n","        best_states['best_val_auc'] = best_val_auc\n","        torch.save(best_states, best_model_path)    # save imediately, the dictionary stores the pointers only not deepcopies\n","        log_print(f\"===Model Updated===\", log_file)\n","    else:\n","        patience_counter += 1\n","        if args.earlystop and patience_counter >= args.early_stop_patience:\n","            log_print(f\"Early stopping at epoch {epoch+1:03d} due to no improvement in Val AUC.\", log_file)\n","            break\n","\n","# track the time, print and write into log file\n","elapsed = time.time() - start_time\n","log_print(f\"\\nTotal training time: {elapsed:.2f} seconds\", log_file)\n","\n","# Save the final model state (last epoch)\n","last_states = save_best_states(args, rr_predictor, attention_pooler,\n","    label_model=label_model,  n2v_model=n2v_model)\n","last_states['val_auc'] = score  # record final val/train AUC for traceability\n","torch.save(last_states, last_model_path)\n","log_print(f\"Last model saved to {last_model_path} with ROC AUC: {score:.4f}\", log_file)\n","\n","# finish and printe the best model's auc\n","log_print(f\"Best model saved to {best_model_path} with Val ROC AUC: {best_val_auc:.4f}\", log_file)\n","# log_file.close()"]},{"cell_type":"markdown","metadata":{"id":"mkPS5RNflDdd"},"source":["## Test with Test Set"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"bWUA6xkQPj16","executionInfo":{"status":"ok","timestamp":1768760530341,"user_tz":300,"elapsed":1,"user":{"displayName":"ccqstd","userId":"13350588400665061144"}}},"outputs":[],"source":["import csv\n","import torch\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, matthews_corrcoef, average_precision_score, roc_curve\n","import torch.nn.functional as F\n","\n","def predict_disease_pairs(args, label_model, n2v_model, attention_pooler, rr_predictor,\n","                          label_data, n2v_data, disease_pair_rr, dis_pairs, device):\n","    \"\"\"\n","      Computes probabilities for disease pairs using the provided models and attention pooling.\n","      Returns:\n","      1. rows for tsv file record\n","      2. ground labels\n","      3. predicted probabilities.\n","    \"\"\"\n","    rows = []\n","    labels = []\n","    probs = []\n","\n","    with torch.no_grad():\n","        # get embeddings from label_model / n2v_model / fusion_model\n","        node_embeddings = compute_node_embeddings(args, label_model, n2v_model, label_data, n2v_data)\n","\n","        for i, (gene_list, rr_label) in enumerate(disease_pair_rr):\n","            # skips any gene indices that are out of bounds for the available node_embeddings\n","            if max(gene_list) >= node_embeddings.shape[0]:\n","                continue\n","            try:\n","                # disease pair prob for rr\n","                gene_embs = node_embeddings[gene_list]\n","                pooled_emb = attention_pooler(gene_embs)\n","                logit = rr_predictor(pooled_emb.unsqueeze(0))\n","                prob = torch.sigmoid(logit).item()\n","                # record results\n","                probs.append(prob)\n","                # Using labels only for logging and scikit-learn, not fot loss function or model calculation, no need to move to device\n","                labels.append(rr_label)\n","                rows.append({\n","                    \"pair_id\": i,\n","                    \"disease_pair\": \"&\".join(dis_pairs[i]),\n","                    \"label\": int(rr_label),\n","                    \"prob\": prob\n","                })\n","            except:\n","                continue\n","\n","    return rows, labels, probs\n","\n","def calculate_metrics_and_update_rows(probs, labels, rows):\n","    \"\"\"\n","      rows are updated with calculated acc, auc, auprc, mcc and best_thresh\n","    \"\"\"\n","    labels_np = np.array(labels)\n","    probs_np = np.array(probs)\n","\n","    # calculate metrics\n","    try:\n","        fpr, tpr, thresholds = roc_curve(labels_np, probs_np)\n","        j_scores = tpr - fpr\n","        best_thresh = thresholds[j_scores.argmax()]\n","        preds = (probs_np > best_thresh).astype(int)\n","\n","        acc = (preds == labels_np).mean()\n","        mcc = matthews_corrcoef(labels_np, preds)\n","        auprc = average_precision_score(labels_np, probs_np)\n","        auc = roc_auc_score(labels_np, probs_np)\n","    except:\n","        acc = mcc = auc = auprc = best_thresh = float(\"nan\")\n","        preds = np.zeros_like(labels_np)\n","\n","    # update rows\n","    for i, row in enumerate(rows):\n","        row[\"pred\"] = int(probs[i] > best_thresh)\n","        row[\"acc\"] = f\"{acc:.4f}\"\n","        row[\"mcc\"] = f\"{mcc:.4f}\"\n","        row[\"auprc\"] = f\"{auprc:.4f}\"\n","        row[\"roc_auc\"] = f\"{auc:.4f}\"\n","        row[\"best_thresh\"] = f\"{best_thresh:.4f}\"\n","\n","    msg = (f\"\\nTest — Best Threshold (J): {best_thresh:.4f} | \"\n","      f\"Acc: {acc:.4f} | ROC AUC: {auc:.4f} | MCC: {mcc:.4f} | AUPRC: {auprc:.4f}\")\n","    log_print(msg, log_file)\n","\n","def save_predictions_to_tsv(rows, output_file):\n","    \"\"\"\n","      save prediction results and metrics to a tsv file.\n","\n","      Args:\n","          rows (list of dict): Prediction records, each with keys like 'prob', 'label', 'acc', etc.\n","          output_file (str): Path to the output .tsv file\n","    \"\"\"\n","    fieldnames = list(rows[0].keys())\n","    with open(output_file, 'w', newline='') as f:\n","        writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='\\t')\n","        writer.writeheader()\n","        writer.writerows(rows)\n","\n","    log_print(f\"Saved test predictions and metrics to {output_file}\", log_file)\n","\n","def load_model_from_checkpoint(save_path, device, args,\n","                               rr_predictor, attention_pooler,\n","                               label_model=None, n2v_model=None):\n","    \"\"\"\n","      loads model weights from a saved checkpoint.\n","    \"\"\"\n","    checkpoint = torch.load(save_path, map_location=device, weights_only=False)\n","    print(\"Checkpoint keys:\", checkpoint.keys())\n","\n","    rr_predictor.load_state_dict(checkpoint['predictor_state_dict'])\n","    attention_pooler.load_state_dict(checkpoint['pooler_state_dict'])\n","\n","    if args.model_type =='label' and label_model:\n","        label_model.load_state_dict(checkpoint['label_model_state_dict'])\n","    if args.model_type =='n2v' and n2v_model:\n","        n2v_model.load_state_dict(checkpoint['n2v_model_state_dict'])\n","\n","    log_print(f\"Model loaded from checkpoint: {save_path}\", log_file)\n","\n","def evaluate_on_test_set_with_best_model_and_save(args, label_model, n2v_model, attention_pooler,\n","                                                  rr_predictor, label_data, n2v_data, test_disease_pair_rr,\n","                                                  test_dis_pairs, device, model_type, val_set, save_path,\n","                                                  output_file):\n","    \"\"\"\n","      the data and models are moved to the device already in the prep data and model step\n","    \"\"\"\n","\n","    # load best model checkpoint, so that no longer using the model in the last epoch\n","    load_model_from_checkpoint(save_path, device, args, rr_predictor, attention_pooler,\n","                                label_model, n2v_model)\n","    # set models to eval mode\n","    set_model_mode('eval', args, label_model, n2v_model, rr_predictor, attention_pooler)\n","\n","    # check after loading best model, make sure valid roc_auc match the best one in log\n","    val_acc, val_auc = evaluate_on_validation(args, label_model, n2v_model,\n","                  rr_predictor, attention_pooler, label_data, n2v_data, val_set, device)\n","\n","    log_print(f\"Reloaded model Val ROC AUC: {val_auc:.4f}\", log_file)\n","\n","    # make predictions\n","    rows, labels, probs = predict_disease_pairs(args, label_model, n2v_model, attention_pooler,\n","                            rr_predictor, label_data, n2v_data, test_disease_pair_rr, test_dis_pairs, device)\n","\n","    if not labels:      # empty label list\n","        print(\"!! No valid predictions.\")\n","        return\n","\n","    # calcualte acc, mcc, rocauc, auprc, and update rows\n","    calculate_metrics_and_update_rows(probs, labels, rows)\n","\n","    # write to file\n","    save_predictions_to_tsv(rows, output_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eesPgiREmwF3"},"outputs":[],"source":["print(best_model_path)\n","print(best_model_output_file)\n","evaluate_on_test_set_with_best_model_and_save(\n","    args, label_model, n2v_model, attention_pooler, rr_predictor,\n","    label_data, n2v_data, test_disease_pair_rr, test_dis_pairs, device,\n","    args.model_type, val_set, best_model_path, best_model_output_file\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppnCEYwup3RL"},"outputs":[],"source":["print(last_model_path)\n","evaluate_on_test_set_with_best_model_and_save(\n","    args, label_model, n2v_model, attention_pooler, rr_predictor,\n","    label_data, n2v_data, test_disease_pair_rr, test_dis_pairs, device,\n","    args.model_type, val_set, last_model_path, last_model_output_file\n",")\n","log_file.close()\n"]},{"cell_type":"markdown","metadata":{"id":"mRNwmdYEH7_s"},"source":["# Disconnect after Done with Train and Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jjNY2MsaUXp"},"outputs":[],"source":["time.sleep(600)  # Wait 5min to ensure all things finish\n","# Disconnect runtime\n","from IPython.display import Javascript\n","display(Javascript('google.colab.kernel.disconnect();'))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["0quiQZnHBMnE","K9guWYYibgve","YdFl3x2SBBFf"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}